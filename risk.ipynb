{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfe3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b8fe8",
   "metadata": {},
   "source": [
    "The below block sets up the main settings for the model (data file, target column, split size, and columns to drop to avoid cheating) and defines how probabilities will be converted into Low, Medium, or High risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b868131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset file\n",
    "CSV_PATH = \"Breast Cancer METABRIC.csv\"\n",
    "\n",
    "# This is the column the model will try to predict\n",
    "TARGET_COL = \"Risk_Level\"\n",
    "\n",
    "# Makes results reproducible (same split every run)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# 20% of data will be used for testing\n",
    "TEST_SIZE = 0.20\n",
    "\n",
    "# # Remove columns that directly reveal survival outcome (prevents data leakage)\n",
    "DROP_COLS_IF_PRESENT = [\"Overall Survival Months\"]\n",
    "\n",
    "\n",
    "# Function to convert predicted probability into risk category\n",
    "def to_risk_bucket(p):\n",
    "    # If probability is low → Low Risk\n",
    "    if p < 0.33:\n",
    "        return \"Low\"\n",
    "    # If probability is moderate → Medium Risk\n",
    "    elif p < 0.66:\n",
    "        return \"Medium\"\n",
    "    # If probability is high → High Risk\n",
    "    return \"High\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "260d548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Patient ID', 'Age at Diagnosis', 'Type of Breast Surgery', 'Cancer Type', 'Cancer Type Detailed', 'Cellularity', 'Chemotherapy', 'Pam50 + Claudin-low subtype', 'Cohort', 'ER status measured by IHC', 'ER Status', 'Neoplasm Histologic Grade', 'HER2 status measured by SNP6', 'HER2 Status', 'Tumor Other Histologic Subtype', 'Hormone Therapy', 'Inferred Menopausal State', 'Integrative Cluster', 'Primary Tumor Laterality', 'Lymph nodes examined positive', 'Mutation Count', 'Nottingham prognostic index', 'Oncotree Code', 'Overall Survival (Months)', 'Overall Survival Status', 'PR Status', 'Radio Therapy', 'Relapse Free Status (Months)', 'Relapse Free Status', 'Sex', '3-Gene classifier subtype', 'Tumor Size', 'Tumor Stage', \"Patient's Vital Status\"]\n",
      "Risk level distribution:\n",
      "Risk_Level\n",
      "Medium    661\n",
      "High      660\n",
      "Low       660\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the CSV file\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Print all column names (helps verify exact column spelling)\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Remove duplicate patient records\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Create 3 risk groups based on survival months\n",
    "# q=3 splits the data into 3 equal-sized groups (quantiles)\n",
    "df[\"Risk_Level\"] = pd.qcut(\n",
    "    df[\"Overall Survival (Months)\"],  # column used to define risk\n",
    "    q=3,                              # divide into 3 equal groups\n",
    "    labels=[\"High\", \"Medium\", \"Low\"]  # shortest survival → High risk\n",
    ")\n",
    "\n",
    "# Print how many patients fall into each risk category\n",
    "print(\"Risk level distribution:\")\n",
    "print(df[\"Risk_Level\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove survival-related columns so the model cannot cheat\n",
    "# (We already used survival months to create Risk_Level)\n",
    "df = df.drop(columns=[\"Overall Survival (Months)\", \"Overall Survival Status\"])\n",
    "\n",
    "# Drop any additional leakage columns if they exist in the dataset\n",
    "df = df.drop(columns=[c for c in DROP_COLS_IF_PRESENT if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "# Remove rows where the target (Risk_Level) is missing\n",
    "df = df.dropna(subset=[TARGET_COL]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ca456f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If target isn't numeric 0/1, map it\n",
    "# if df[TARGET_COL].dtype == \"object\":\n",
    "#     classes = sorted(df[TARGET_COL].dropna().unique())\n",
    "#     if len(classes) != 2:\n",
    "#         raise ValueError(f\"Target is not binary. Found: {classes}\")\n",
    "#     mapping = {classes[0]: 0, classes[1]: 1}\n",
    "#     df[TARGET_COL] = df[TARGET_COL].map(mapping)\n",
    "\n",
    "\n",
    "# Set target column to Risk_Level (Low/Medium/High)\n",
    "TARGET_COL = \"Risk_Level\"\n",
    "\n",
    "# Ensure there are no missing risk labels\n",
    "df = df.dropna(subset=[TARGET_COL]).copy()\n",
    "\n",
    "# Convert risk labels into numeric classes for XGBoost\n",
    "# Low = 0, Medium = 1, High = 2\n",
    "df[TARGET_COL] = df[TARGET_COL].map({\n",
    "    \"Low\": 0,\n",
    "    \"Medium\": 1,\n",
    "    \"High\": 2\n",
    "}).astype(int)\n",
    "\n",
    "\n",
    "# Feature engineering: create nonlinear age feature\n",
    "# Helps model capture nonlinear age-risk relationships\n",
    "if \"Age at Diagnosis\" in df.columns:\n",
    "    df[\"Age_Squared\"] = df[\"Age at Diagnosis\"] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a04221b",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets, preprocess the features (handle missing values and convert text to numbers), and train an XGBoost model to predict the risk category.\n",
    "\n",
    "Then you evaluate how well the model performs by checking its accuracy, detailed classification metrics, and confusion matrix on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "239679c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9093198992443325\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92       132\n",
      "           1       0.86      0.91      0.89       133\n",
      "           2       0.91      0.93      0.92       132\n",
      "\n",
      "    accuracy                           0.91       397\n",
      "   macro avg       0.91      0.91      0.91       397\n",
      "weighted avg       0.91      0.91      0.91       397\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[117  11   4]\n",
      " [  4 121   8]\n",
      " [  1   8 123]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SPLIT\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# PREPROCESS\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
    "        (\"cat\", Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# MODEL (XGBoost)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", xgb)\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# EVALUATE (binary)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d54f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: risk_predictions_test.csv\n",
      "\n",
      "Risk bucket counts:\n",
      " Low       250\n",
      "High      133\n",
      "Medium     14\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# RISK BUCKETS (Low/Med/High)\n",
    "proba_class1 = model.predict_proba(X_test)[:, 1]\n",
    "risk_bucket = [to_risk_bucket(p) for p in proba_class1]\n",
    "\n",
    "out = X_test.copy()\n",
    "out[\"y_true\"] = y_test.values\n",
    "out[\"pred_prob_class1\"] = proba_class1\n",
    "out[\"risk_bucket\"] = risk_bucket\n",
    "\n",
    "out_path = \"risk_predictions_test.csv\"\n",
    "out.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\nSaved:\", out_path)\n",
    "print(\"\\nRisk bucket counts:\\n\", pd.Series(risk_bucket).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1934b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"risk_predictions_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06ea156a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Low\n",
       "1     Low\n",
       "2    High\n",
       "3     Low\n",
       "4     Low\n",
       "Name: risk_bucket, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"risk_bucket\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9fb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
